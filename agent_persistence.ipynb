{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistence and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The current weather in Ankara is partly cloudy with a temperature of 4.3¬∞C (39.7¬∞F). There's a southeast wind blowing at 3.6 kph (2.2 mph), and the humidity is 100%.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 793, 'total_tokens': 843, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd035d83-4a16-40bd-9b2d-30b6c9665fab-0', usage_metadata={'input_tokens': 793, 'output_tokens': 50, 'total_tokens': 843, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "messages = [HumanMessage(content=\"What is the weather in Ankara?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"It seems there might be some confusion, as you've repeated the question about the weather in Ankara. Could you please specify if there's anything else you're looking for regarding Ankara's weather or any other information about the city?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 854, 'total_tokens': 898, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None}, id='run-b677a79e-f1f1-44f0-991a-a43320bf7cbd-0', usage_metadata={'input_tokens': 854, 'output_tokens': 44, 'total_tokens': 898, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in Ankara?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zh6GInUeKj1WMRf0RHbT5VyZ', 'function': {'arguments': '{\"query\":\"Ankara weather forecast October 7 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 151, 'total_tokens': 179, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f739c39a-003e-434c-b082-889bcb0bda74-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Ankara weather forecast October 7 2023'}, 'id': 'call_zh6GInUeKj1WMRf0RHbT5VyZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 28, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'Ankara weather forecast October 7 2023'}, 'id': 'call_zh6GInUeKj1WMRf0RHbT5VyZ', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Ankara\\', \\'region\\': \\'Ankara\\', \\'country\\': \\'Turkey\\', \\'lat\\': 39.9272, \\'lon\\': 32.8644, \\'tz_id\\': \\'Europe/Istanbul\\', \\'localtime_epoch\\': 1736961304, \\'localtime\\': \\'2025-01-15 20:15\\'}, \\'current\\': {\\'last_updated_epoch\\': 1736961300, \\'last_updated\\': \\'2025-01-15 20:15\\', \\'temp_c\\': 3.3, \\'temp_f\\': 37.9, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 2.2, \\'wind_kph\\': 3.6, \\'wind_degree\\': 196, \\'wind_dir\\': \\'SSW\\', \\'pressure_mb\\': 1019.0, \\'pressure_in\\': 30.09, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 100, \\'cloud\\': 75, \\'feelslike_c\\': 2.8, \\'feelslike_f\\': 37.1, \\'windchill_c\\': 6.7, \\'windchill_f\\': 44.0, \\'heatindex_c\\': 6.7, \\'heatindex_f\\': 44.0, \\'dewpoint_c\\': 3.7, \\'dewpoint_f\\': 38.6, \\'vis_km\\': 4.0, \\'vis_miles\\': 2.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.7, \\'gust_kph\\': 10.8}}\"}, {\\'url\\': \\'https://world-weather.info/forecast/turkey/ankara/october-2023/\\', \\'content\\': \\'Detailed ‚ö° Ankara Weather Forecast for October 2023 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info\\'}]', name='tavily_search_results_json', tool_call_id='call_zh6GInUeKj1WMRf0RHbT5VyZ')]\n",
      "[AIMessage(content='The current weather in Ankara includes a temperature of approximately 3.3¬∞C (37.9¬∞F) with mist. The wind is blowing from the south-southwest at 3.6 kph (2.2 mph), and the humidity level is at 100%, with visibility around 4 kilometers.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 646, 'total_tokens': 711, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None}, id='run-30a43dd7-b266-45bd-8d2e-2bdc0d299aad-0', usage_metadata={'input_tokens': 646, 'output_tokens': 65, 'total_tokens': 711, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in Ankara?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"Could you please clarify what specific information you are looking for about Ankara? Whether it's additional weather details or another aspect of the city you're interested in, let me know so I can assist you further.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 722, 'total_tokens': 763, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None}, id='run-05acfe11-cfa7-4341-a120-4a14494dc0b8-0', usage_metadata={'input_tokens': 722, 'output_tokens': 41, 'total_tokens': 763, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in Ankara?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Here are the questions you asked about Ankara:\\n\\n1. \"What is the weather in Ankara?\"\\n2. \"What about in Ankara?\" (This was a bit unclear, so I asked for clarification.)\\n\\nIf you meant something specific with your second question, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 782, 'total_tokens': 839, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0f8c83e59b', 'finish_reason': 'stop', 'logprobs': None}, id='run-f95b9ef7-8b7f-4ef2-803c-6db2101ebf66-0', usage_metadata={'input_tokens': 782, 'output_tokens': 57, 'total_tokens': 839, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which questions did I ask you about Ankara? List them all.\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_alive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in Ankara?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      9\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m abot\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages}, thread, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     kind \u001b[38;5;241m=\u001b[39m event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_chat_model_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langchain_core\\runnables\\base.py:1386\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[1;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[1;32m-> 1386\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[0;32m   1387\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langchain_core\\tracers\\event_stream.py:781\u001b[0m, in \u001b[0;36m_astream_events_implementation_v1\u001b[1;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, runnable\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     runnable,\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    784\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    785\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    786\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    787\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    789\u001b[0m ):\n\u001b[0;32m    790\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langchain_core\\tracers\\log_stream.py:675\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[1;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39msuppress(asyncio\u001b[38;5;241m.\u001b[39mCancelledError):\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langchain_core\\tracers\\log_stream.py:629\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[1;34m()\u001b[0m\n\u001b[0;32m    626\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    627\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    630\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1836\u001b[0m, in \u001b[0;36mPregel.astream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_modes:\n\u001b[0;32m   1831\u001b[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1832\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m c: aioloop\u001b[38;5;241m.\u001b[39mcall_soon_threadsafe(\n\u001b[0;32m   1833\u001b[0m             stream\u001b[38;5;241m.\u001b[39mput_nowait, ((), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n\u001b[0;32m   1834\u001b[0m         )\n\u001b[0;32m   1835\u001b[0m     )\n\u001b[1;32m-> 1836\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncPregelLoop(\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1838\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput_nowait, stream_modes),\n\u001b[0;32m   1839\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1840\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[0;32m   1841\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[0;32m   1842\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[0;32m   1843\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[0;32m   1844\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1845\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[0;32m   1846\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1847\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1848\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1849\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1850\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[0;32m   1852\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[0;32m   1853\u001b[0m         submit\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m   1854\u001b[0m         put_writes\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39mput_writes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[0;32m   1858\u001b[0m     )\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langgraph\\pregel\\loop.py:988\u001b[0m, in \u001b[0;36mAsyncPregelLoop.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CheckpointNotLatest\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer:\n\u001b[1;32m--> 988\u001b[0m     saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\u001b[38;5;241m.\u001b[39maget_tuple(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_config)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langgraph\\checkpoint\\sqlite\\aio.py:304\u001b[0m, in \u001b[0;36mAsyncSqliteSaver.aget_tuple\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[CheckpointTuple]:\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a checkpoint tuple from the database asynchronously.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This method retrieves a checkpoint tuple from the SQLite database based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m        Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m    305\u001b[0m     checkpoint_ns \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_ns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;66;03m# find the latest checkpoint for the thread_id\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaan\\anaconda3\\envs\\agenticai_stable\\lib\\site-packages\\langgraph\\checkpoint\\sqlite\\aio.py:258\u001b[0m, in \u001b[0;36mAsyncSqliteSaver.setup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_setup:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_alive\u001b[49m():\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mexecutescript(\n\u001b[0;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    PRAGMA journal_mode=WAL;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    285\u001b[0m ):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_alive'"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "import asyncio\n",
    "\n",
    "memory = AsyncSqliteSaver(\":memory:\")\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"What is the weather in Ankara?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "        kind = event.get(\"event\", \"\")\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of OpenAI means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
